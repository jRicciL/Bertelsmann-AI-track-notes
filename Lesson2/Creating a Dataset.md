# Creating a Dataset

## Data Fit and Annotation
### Answering questions with data
#### Building the dataset
- **Is the dataset complete?**
- **Does the dataset fit the problem?**

#### Ensuring Data Fit
- Use production data => Generated by users
	- To ensure the training data matches real-world scenarios
	- Data should represent the problem
- Determine the success criteria for a trained model
	- Confusion Matrix
	- Accuracy: Number of Trues/n
	- Precision:
		- **Positive predictive Value**
		- Jugar a la segura
			- Try that the majority of my positives will be True Positives
			- Cost: It's ok if I miss some positives 
		- The fraction of **my positives** that are true positives
		- TP/(TP + FP)
	- Recall: 
		- **True Positive Rate**
			- Try to find all the positives possible
			- Cost: It's ok if I got some False Positives
		- How many of **my positives** are true positives
		- Number of TP/(TP and FN) = TP/P
	- F1 score => A combination of P and R

### Data Completeness
- Collect data and observe relationships
- Identify
	- Anomalies
	- Missing data

### Data Annotation
- Data often comes in a tabular format

##### Data Annotation Platforms
- Appen.com (Figure Eight) is a platform to perform data annotation
	- Let us to label text, speech, image data, and more
	- ![](https://video.udacity-data.com/topher/2019/May/5cdcbb0f_screen-shot-2019-05-15-at-6.21.03-pm/screen-shot-2019-05-15-at-6.21.03-pm.png)
- https://client.appen.com/welcome

### Case Study with Appen.com
- **Data annotation platform**
	- Uploading data
	- Designing an annotation job
	- Creating test questions
	- Monitoring results

#### Sections
	
##### DESIGN: Annotation instructions
- At least one example for each possible data annotation
- Examples of ambiguous or "tricky" annotation cases

##### QUALITY: Test Questions
- Create a set of questions (already labeled data) to evaluate if the contributors are able to do the task correctly. 
- Evaluate the accuracy of the contributor answers.
- We have the chance to evaluate which questions are more challenging and to use them as "tricky" examples

##### LAUNCH: 
- Launch only a subset of the data

### Planning for Failure and Future
- If your data does not change, you can use static model
- for ever-evolving data, we should use a dynamic model
	- Continuously trained on new data
- We may need to change the annotation job and update the data